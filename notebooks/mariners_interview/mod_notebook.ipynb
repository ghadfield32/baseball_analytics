{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. data_loading.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/mariners_interview/data_loading.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/mariners_interview/data_loading.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(train_path, test_path, dict_path, debug=False):\n",
    "    \"\"\"\n",
    "    Load the training and testing datasets.\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    data_dict_df = pd.read_csv(dict_path)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Train Data Shape:\", train_df.head())\n",
    "        print(\"Test Data Shape:\", test_df.head())\n",
    "        print(\"Data Dictionary Shape:\", data_dict_df)\n",
    "    \n",
    "    return train_df, test_df, data_dict_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Paths to data files\n",
    "    train_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-train.csv'\n",
    "    test_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-test.csv'\n",
    "    dict_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-dictionary.csv'\n",
    "    \n",
    "    # Load data with debug mode enabled\n",
    "    train_df, test_df, data_dict_df = load_data(train_path, test_path, dict_path, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/mariners_interview/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/mariners_interview/preprocessing.py\n",
    "\n",
    "def clean_data(df, debug=False):\n",
    "    if debug:\n",
    "        print(f\"DataFrame shape after loading data: {df.shape}\")\n",
    "        print(f\"Columns before cleaning: {df.columns.tolist()}\")\n",
    "    # Assuming the cleaning process doesn't drop the 'inning' column\n",
    "    if debug:\n",
    "        print(f\"Columns after cleaning: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df, debug=False):\n",
    "    if debug:\n",
    "        print(f\"Columns before handling missing values: {df.columns.tolist()}\")\n",
    "    # Example of a missing value handling process\n",
    "    df = df[df['hit_spin_rate'].notnull()]  # Assuming hit_spin_rate is critical and should not have nulls\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Columns after handling missing values: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # import pandas as pd\n",
    "    # from data_loading import load_data\n",
    "    \n",
    "    # Load data for testing\n",
    "    train_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-train.csv'\n",
    "    test_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-test.csv'\n",
    "    dict_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-dictionary.csv'\n",
    "    \n",
    "    # Load data with debug mode enabled\n",
    "    train_df, test_df, data_dict_df = load_data(train_path, test_path, dict_path, debug=True)\n",
    "    \n",
    "    # Clean data with debug mode enabled\n",
    "    train_df = clean_data(train_df, debug=True)\n",
    "    train_df = handle_missing_values(train_df, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. feature_engineering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/mariners_interview/feature_engineering.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/mariners_interview/feature_engineering.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_hit_trajectory(vert_exit_angle, horz_exit_angle, exit_speed, spin_rate, gravity=32.174, time_intervals=100):\n",
    "    \"\"\"\n",
    "    Calculate the hit trajectory based on initial speed and angles.\n",
    "    Args:\n",
    "    - vert_exit_angle: Vertical exit angle in degrees.\n",
    "    - horz_exit_angle: Horizontal exit angle in degrees.\n",
    "    - exit_speed: Initial exit speed in mph.\n",
    "    - spin_rate: Spin rate in rpm.\n",
    "    - gravity: Gravity constant in ft/s^2.\n",
    "    - time_intervals: Number of time intervals for calculating the trajectory.\n",
    "\n",
    "    Returns:\n",
    "    - x_values: List of x coordinates of the trajectory.\n",
    "    - y_values: List of y coordinates of the trajectory.\n",
    "    \"\"\"\n",
    "    # Convert exit speed to ft/s (1 mph = 1.46667 ft/s)\n",
    "    exit_speed_ft_s = exit_speed * 1.46667\n",
    "\n",
    "    # Convert angles to radians\n",
    "    vert_angle_rad = np.radians(vert_exit_angle)\n",
    "    horz_angle_rad = np.radians(horz_exit_angle)\n",
    "\n",
    "    # Calculate initial velocity components\n",
    "    vx = exit_speed_ft_s * np.cos(vert_angle_rad)\n",
    "    vy = exit_speed_ft_s * np.sin(vert_angle_rad)\n",
    "\n",
    "    # Calculate the effect of spin on horizontal and vertical distances\n",
    "    # Approximate adjustment factor due to spin rate\n",
    "    spin_effect = 1 + (spin_rate / 15000)\n",
    "\n",
    "    # Calculate trajectory points\n",
    "    time_points = np.linspace(0, 5, time_intervals)  # 5 seconds max trajectory time\n",
    "    x_values = (vx * time_points) * np.cos(horz_angle_rad) * spin_effect\n",
    "    y_values = (vy * time_points - 0.5 * gravity * time_points ** 2) * spin_effect\n",
    "\n",
    "    # Remove any points where y is negative (ground level or below)\n",
    "    valid_indices = y_values >= 0\n",
    "    x_values = x_values[valid_indices]\n",
    "    y_values = y_values[valid_indices]\n",
    "\n",
    "    return x_values, y_values\n",
    "\n",
    "def visualize_hit_trajectory(df, boundary=None, debug=False):\n",
    "    \"\"\"\n",
    "    Visualize the hit's trajectory and landing point with respect to the field boundaries.\n",
    "    Ensure that the boundary and trajectory visualization match the calculation methods.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    # Simulate outfield boundary using the same function and parameters as used for determining home runs\n",
    "    if boundary is None:\n",
    "        boundary = simulate_outfield_boundary(average_distances, debug=debug)\n",
    "\n",
    "    boundary_x, boundary_y, _, _ = boundary\n",
    "\n",
    "    # Visualize the outfield boundary\n",
    "    ax.plot(boundary_x, boundary_y, color='black', linestyle='--', linewidth=2, label='Outfield Boundary')\n",
    "\n",
    "    # Plot each hit's trajectory\n",
    "    for idx, row in df.iterrows():\n",
    "        # Calculate the trajectory using the initial speed, angles, and spin rate\n",
    "        x_traj, y_traj = calculate_hit_trajectory(\n",
    "            row['vert_exit_angle'],\n",
    "            row['horz_exit_angle'],\n",
    "            row['exit_speed'],\n",
    "            row['hit_spin_rate']\n",
    "        )\n",
    "\n",
    "        # Plot trajectory line\n",
    "        ax.plot(x_traj, y_traj, linestyle='-', alpha=0.6)\n",
    "\n",
    "        # Plot the landing point\n",
    "        ax.scatter(row['landing_x_adjusted'], row['landing_y_adjusted'], color='red', s=50, label='Landing Point' if idx == 0 else \"\")\n",
    "\n",
    "    # Set field boundary limits to show the full field\n",
    "    max_x = max(boundary_x) + 50\n",
    "    max_y = max(boundary_y) + 50\n",
    "    ax.set_xlim(-max_x, max_x)\n",
    "    ax.set_ylim(0, max_y)\n",
    "\n",
    "    # Formatting the plot\n",
    "    ax.set_title(\"Hit Trajectory and Landing Points with Outfield Boundary\")\n",
    "    ax.set_xlabel(\"Landing X (ft)\")\n",
    "    ax.set_ylabel(\"Landing Y (ft)\")\n",
    "    ax.axhline(0, color='black', linewidth=1)  # Baseline for y-axis\n",
    "    ax.axvline(0, color='black', linewidth=1)  # Baseline for x-axis\n",
    "    ax.legend(title='Legend')\n",
    "    ax.grid(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "#https://www.si.com/mlb/2021/03/24/mlb-outfield-walls-ranked-fenway-park-yankee-stadium\n",
    "\n",
    "# Provided data\n",
    "stadium_data = {\n",
    "    'Kauffman Stadium': [330, 387, 410, 387, 330],\n",
    "    'Rogers Centre': [328, 375, 400, 375, 328],\n",
    "    'TD Ballpark': [333, 380, 400, 363, 336],\n",
    "    'Busch Stadium': [336, 375, 400, 375, 335],\n",
    "    'Dodger Stadium': [330, 360, 375, 400, 375, 360, 330],\n",
    "    'Guaranteed Rate Field': [330, 375, 400, 375, 335],\n",
    "    'Oakland Coliseum': [330, 388, 400, 388, 330],\n",
    "    'Marlins Park': [344, 386, 400, 387, 335],\n",
    "    'Miller Park': [344, 371, 400, 374, 345],\n",
    "    'T-Mobile Park': [331, 378, 401, 381, 326],\n",
    "    'Citi Field': [335, 358, 385, 408, 398, 375, 330],\n",
    "    'Tropicana Field': [315, 370, 404, 370, 322],\n",
    "    'Truist Park': [335, 385, 400, 375, 325],\n",
    "    'Wrigley Field': [355, 368, 400, 368, 353],\n",
    "    'Coors Field': [347, 390, 415, 375, 350],\n",
    "    'Angel Stadium': [347, 390, 396, 370, 365, 350],\n",
    "    'Comerica Park': [345, 370, 420, 365, 330],\n",
    "    'Great American Ball Park': [328, 379, 404, 370, 325],\n",
    "    'Nationals Park': [337, 377, 402, 370, 335],\n",
    "    'Progressive Field': [325, 370, 400, 410, 375, 325],\n",
    "    'Target Field': [339, 377, 411, 403, 367, 328],\n",
    "    'Oriole Park at Camden Yards': [333, 364, 410, 400, 373, 318],\n",
    "    'Chase Field': [330, 374, 413, 407, 413, 374, 334],\n",
    "    'Globe Life Field': [329, 372, 407, 374, 326],\n",
    "    'Petco Park': [334, 357, 390, 396, 391, 382, 322],\n",
    "    'Citizens Bank Park': [329, 374, 409, 401, 369, 330],\n",
    "    'Yankee Stadium': [318, 399, 408, 385, 314],\n",
    "    'PNC Park': [325, 383, 410, 399, 375, 320],\n",
    "    'Minute Maid Park': [315, 362, 404, 409, 408, 373, 326],\n",
    "    'Oracle Park': [339, 364, 399, 391, 415, 365, 309],\n",
    "    'Fenway Park': [310, 379, 390, 420, 380, 302]\n",
    "}\n",
    "\n",
    "# Initialize an empty list to collect data\n",
    "data = []\n",
    "\n",
    "for stadium, distances in stadium_data.items():\n",
    "    num_points = len(distances)\n",
    "    if num_points == 5:\n",
    "        # Map directly\n",
    "        P1 = distances[0]  # Left Field Line\n",
    "        P2 = distances[1]  # Left-Center Field\n",
    "        P3 = distances[2]  # Center Field\n",
    "        P4 = distances[3]  # Right-Center Field\n",
    "        P5 = distances[4]  # Right Field Line\n",
    "    elif num_points == 6:\n",
    "        # Use positions 0,1,2,3,5\n",
    "        P1 = distances[0]\n",
    "        P2 = distances[1]\n",
    "        P3 = distances[2]\n",
    "        P4 = distances[3]\n",
    "        P5 = distances[5]\n",
    "    elif num_points == 7:\n",
    "        # Use positions 0,2,3,4,6\n",
    "        P1 = distances[0]\n",
    "        P2 = distances[2]\n",
    "        P3 = distances[3]\n",
    "        P4 = distances[4]\n",
    "        P5 = distances[6]\n",
    "    else:\n",
    "        # Handle other cases if necessary\n",
    "        continue  # Skip if the number of distances is not 5,6,7\n",
    "\n",
    "    data.append({\n",
    "        'Stadium': stadium,\n",
    "        'P1': P1,\n",
    "        'P2': P2,\n",
    "        'P3': P3,\n",
    "        'P4': P4,\n",
    "        'P5': P5\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate averages\n",
    "average_P1 = df['P1'].mean()\n",
    "average_P2 = df['P2'].mean()\n",
    "average_P3 = df['P3'].mean()\n",
    "average_P4 = df['P4'].mean()\n",
    "average_P5 = df['P5'].mean()\n",
    "\n",
    "# Display the DataFrame and averages\n",
    "print(\"Stadium Distances DataFrame:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\nAverage Distances:\")\n",
    "print(f\"Left Field Line (P1): {average_P1:.2f} ft\")\n",
    "print(f\"Left-Center Field (P2): {average_P2:.2f} ft\")\n",
    "print(f\"Center Field (P3): {average_P3:.2f} ft\")\n",
    "print(f\"Right-Center Field (P4): {average_P4:.2f} ft\")\n",
    "print(f\"Right Field Line (P5): {average_P5:.2f} ft\")\n",
    "\n",
    "\n",
    "\n",
    "def categorize_inning(df, debug=False):\n",
    "    if debug:\n",
    "        print(f\"Columns before categorizing innings: {df.columns.tolist()}\")\n",
    "        print(f\"Checking if 'inning' exists in the DataFrame: {'inning' in df.columns}\")\n",
    "        if 'inning' not in df.columns:\n",
    "            print(\"ERROR: 'inning' column is missing. Exiting function early.\")\n",
    "            return df  # Exit early if the 'inning' column is missing\n",
    "\n",
    "    # Proceed to categorize 'inning' only if it exists\n",
    "    if 'inning' in df.columns:\n",
    "        df['inning_group'] = pd.cut(df['inning'], bins=[0, 3, 6, np.inf], labels=['Early', 'Mid', 'Late'])\n",
    "        df = df.drop(columns=['inning'])  # Drop the original inning column if not needed\n",
    "    if debug:\n",
    "        print(f\"Columns after categorizing innings: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "def create_count_scenario(df, debug=False):\n",
    "    \"\"\"\n",
    "    Combine 'pre_balls', 'pre_strikes', 'pre_outs', and 'inning_group' into 'count_scenario'.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Columns before create_count_scenario: {df.columns.tolist()}\")\n",
    "    # Check for required columns before proceeding\n",
    "    required_columns = ['pre_balls', 'pre_strikes', 'pre_outs', 'inning_group']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"Missing required column: {col}. Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Combine game context features into a single string representation\n",
    "    df['count_scenario'] = (df['pre_balls'].astype(str) + '-' + \n",
    "                            df['pre_strikes'].astype(str) + '-' +\n",
    "                            df['pre_outs'].astype(str) + '-' +\n",
    "                            df['inning_group'].astype(str))\n",
    "    \n",
    "    # Drop the original columns if not needed anymore\n",
    "    df = df.drop(columns=['pre_balls', 'pre_strikes', 'pre_outs', 'inning_group'])\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Unique 'count_scenario's:\", df['count_scenario'].unique())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_gamedate(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"Transforming 'gamedate' column...\")\n",
    "    df['gamedate'] = pd.to_datetime(df['gamedate'])\n",
    "    df['month'] = df['gamedate'].dt.month\n",
    "    df['day_of_week'] = df['gamedate'].dt.day_name()\n",
    "    df['is_weekend'] = df['gamedate'].dt.dayofweek >= 5\n",
    "    df = df.drop(columns=['gamedate'])\n",
    "    if debug:\n",
    "        print(\"Transformed columns:\", df.columns)\n",
    "    return df\n",
    "\n",
    "def categorize_temperature(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"Categorizing 'temperature' column...\")\n",
    "    bins = [0, 70, 90, np.inf]\n",
    "    labels = ['Cold', 'Moderate', 'Hot']\n",
    "    df['temperature_category'] = pd.cut(df['temperature'], bins=bins, labels=labels)\n",
    "    df = df.drop(columns=['temperature'])\n",
    "    if debug:\n",
    "        print(\"Temperature categories assigned:\", df['temperature_category'].unique())\n",
    "    return df\n",
    "\n",
    "def calculate_physics_features(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"Calculating physics-based features...\")\n",
    "    GRAVITY = 32.174  # ft/s^2\n",
    "    df['vert_angle_rad'] = np.radians(df['vert_exit_angle'])\n",
    "    df['estimated_distance'] = ((df['exit_speed'] ** 2) * np.sin(2 * df['vert_angle_rad'])) / GRAVITY\n",
    "    df['landing_x'] = df['estimated_distance'] * np.sin(np.radians(df['horz_exit_angle']))\n",
    "    df['landing_y'] = df['estimated_distance'] * np.cos(np.radians(df['horz_exit_angle']))\n",
    "    df['adjusted_distance'] = df['estimated_distance'] * (1 + (df['hit_spin_rate'] / 15000))\n",
    "    df['landing_x_adjusted'] = df['adjusted_distance'] * np.sin(np.radians(df['horz_exit_angle']))\n",
    "    df['landing_y_adjusted'] = df['adjusted_distance'] * np.cos(np.radians(df['horz_exit_angle']))\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Physics features calculated.\")\n",
    "        print(df[['estimated_distance', 'adjusted_distance', 'landing_x_adjusted', 'landing_y_adjusted']].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def determine_home_run(df, boundary=None, debug=False):\n",
    "    \"\"\"\n",
    "    Determine if each hit in the DataFrame is a home run using the precomputed outfield boundary.\n",
    "    \"\"\"\n",
    "    if boundary is None:\n",
    "        boundary = simulate_outfield_boundary(average_distances, debug=False)  # Use precomputed boundary\n",
    "\n",
    "    boundary_x, boundary_y, _, _ = boundary\n",
    "\n",
    "    def is_home_run(row):\n",
    "        x = row['landing_x_adjusted']\n",
    "        y = row['landing_y_adjusted']\n",
    "        distance = np.sqrt(x**2 + y**2)\n",
    "\n",
    "        # Calculate the angle of the hit\n",
    "        hit_angle = np.degrees(np.arctan2(x, y))\n",
    "\n",
    "        # Find the corresponding boundary distance for this angle\n",
    "        if -45 <= hit_angle <= 45:\n",
    "            # Find the closest point in the boundary array\n",
    "            idx = np.abs(np.linspace(-45, 45, 100) - hit_angle).argmin()\n",
    "            boundary_distance = np.sqrt(boundary_x[idx]**2 + boundary_y[idx]**2)\n",
    "            return distance >= boundary_distance\n",
    "\n",
    "        return False\n",
    "\n",
    "    df['is_home_run'] = df.apply(is_home_run, axis=1)\n",
    "    if debug:\n",
    "        print(\"Number of home runs:\", df['is_home_run'].sum())\n",
    "        print(df[df['is_home_run'] == True][['hit_direction', 'adjusted_distance']].head())\n",
    "\n",
    "    return df\n",
    "\n",
    "global_boundary = None\n",
    "\n",
    "average_distances = {\n",
    "    'P1': 332.45,  # Average Left Field Line\n",
    "    'P2': 381.55,  # Average Left-Center Field\n",
    "    'P3': 403.48,  # Average Center Field\n",
    "    'P4': 385.81,  # Average Right-Center Field\n",
    "    'P5': 329.16   # Average Right Field Line\n",
    "}\n",
    "\n",
    "# Updated function to simulate outfield boundary with debug outputs at every 5 feet\n",
    "def simulate_outfield_boundary(average_distances, debug=False):\n",
    "    \"\"\"\n",
    "    Create a gradual outfield boundary using a polynomial curve fit or similar, using average distances.\n",
    "    Output dimensions once for debugging purposes if debug is True.\n",
    "    \"\"\"\n",
    "    global global_boundary  # Use a global boundary to avoid repeated calculations\n",
    "    \n",
    "    # If the boundary is already calculated, return it directly.\n",
    "    if global_boundary is not None:\n",
    "        return global_boundary\n",
    "\n",
    "    # Extract average distances for each field position\n",
    "    P1 = average_distances['P1']\n",
    "    P2 = average_distances['P2']\n",
    "    P3 = average_distances['P3']\n",
    "    P4 = average_distances['P4']\n",
    "    P5 = average_distances['P5']\n",
    "\n",
    "    # Define angles corresponding to each point\n",
    "    angles = np.linspace(-45, 45, 100)  # Covering left to right field (in degrees)\n",
    "    angles_rad = np.radians(angles)\n",
    "\n",
    "    # Calculate polynomial coefficients based on these points\n",
    "    # Create a smooth curve that fits through (Left Field Line, Left-Center, Center Field, Right-Center, Right Field Line)\n",
    "    boundary_coefficients = np.polyfit(\n",
    "        [-45, -22.5, 0, 22.5, 45],  # angles corresponding to each average point\n",
    "        [P1, P2, P3, P4, P5],  # distance values\n",
    "        deg=3  # Cubic polynomial fit\n",
    "    )\n",
    "\n",
    "    # Generate boundary distances using the fitted polynomial\n",
    "    boundary_distances = np.polyval(boundary_coefficients, angles)\n",
    "\n",
    "    # Calculate boundary x and y positions based on these distances\n",
    "    boundary_x = boundary_distances * np.sin(angles_rad)\n",
    "    boundary_y = boundary_distances * np.cos(angles_rad)\n",
    "\n",
    "    # Generate outfield boundary points every 5 feet for debugging purposes\n",
    "    distances_5ft = np.arange(0, max(boundary_distances), 5)\n",
    "    boundary_x_5ft = []\n",
    "    boundary_y_5ft = []\n",
    "\n",
    "    # Populate boundary values at every 5-foot interval for the outfield\n",
    "    for d in distances_5ft:\n",
    "        # Find the corresponding x and y for this distance\n",
    "        idx = (np.abs(boundary_distances - d)).argmin()\n",
    "        boundary_x_5ft.append(boundary_x[idx])\n",
    "        boundary_y_5ft.append(boundary_y[idx])\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Outfield Boundary at {d} ft: (x={boundary_x[idx]:.2f}, y={boundary_y[idx]:.2f})\")\n",
    "\n",
    "    # Store the calculated boundary in the global variable\n",
    "    global_boundary = (boundary_x, boundary_y, boundary_x_5ft, boundary_y_5ft)\n",
    "\n",
    "    return global_boundary\n",
    "\n",
    "\n",
    "def visualize_in_park_foul_balls(df, title='In-Park Foul Balls'):\n",
    "    \"\"\"\n",
    "    Visualize the in-park foul balls that are catchable.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    in_park_foul_df = df[df['is_foul'] & ~df['is_home_run']]  # Filter only catchable in-park foul balls\n",
    "    \n",
    "    sns.scatterplot(x='landing_x_adjusted', y='landing_y_adjusted', hue='hit_direction', data=in_park_foul_df, palette='deep')\n",
    "    \n",
    "    # Additional visual aids\n",
    "    plt.axhline(0, color='black', linewidth=1)  # Baseline for y-axis\n",
    "    plt.axvline(0, color='black', linewidth=1)  # Baseline for x-axis\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Landing X (ft)')\n",
    "    plt.ylabel('Landing Y (ft)')\n",
    "    plt.legend(title='Hit Direction')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_hits_with_field_boundary(df, title='Hit Landing Points', hits_type='all', debug=False):\n",
    "    \"\"\"\n",
    "    Visualize the landing points of hits with different categories, including the outfield boundary.\n",
    "    \"\"\"\n",
    "    # Generate the field boundary\n",
    "    boundary_x, boundary_y, _, _ = simulate_outfield_boundary(average_distances, debug=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Determine the data to plot based on the hit type\n",
    "    if hits_type == 'all':\n",
    "        data = df\n",
    "    elif hits_type == 'home_runs':\n",
    "        data = df[df['is_home_run']]\n",
    "    elif hits_type == 'foul_balls':\n",
    "        data = df[df['is_foul']]\n",
    "    else:\n",
    "        data = df\n",
    "\n",
    "    # Plot the hit points\n",
    "    sns.scatterplot(x='landing_x_adjusted', y='landing_y_adjusted', hue='hit_direction', data=data, palette='deep')\n",
    "\n",
    "    # Plot the field boundary\n",
    "    plt.plot(boundary_x, boundary_y, color='black', linestyle='--', linewidth=2, label='Outfield Boundary')\n",
    "    \n",
    "    # Additional visual aids\n",
    "    plt.axhline(0, color='black', linewidth=1)  # Baseline for y-axis\n",
    "    plt.axvline(0, color='black', linewidth=1)  # Baseline for x-axis\n",
    "    \n",
    "    # Set the title and labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Landing X (ft)')\n",
    "    plt.ylabel('Landing Y (ft)')\n",
    "    plt.legend(title='Hit Direction')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Modify is_within_field_boundaries to use the simulated boundary curve\n",
    "def is_within_field_boundaries(row, boundary=None, debug=False):\n",
    "    \"\"\"\n",
    "    Check if the ball lands within the field boundaries using the precomputed boundary.\n",
    "    \"\"\"\n",
    "    if boundary is None:\n",
    "        boundary = simulate_outfield_boundary(average_distances, debug=False)  # Use precomputed boundary\n",
    "\n",
    "    boundary_x, boundary_y, boundary_x_5ft, boundary_y_5ft = boundary\n",
    "\n",
    "    x = row['landing_x_adjusted']\n",
    "    y = row['landing_y_adjusted']\n",
    "\n",
    "    # Determine if point is inside the boundary curve by checking the distance to the origin\n",
    "    distance = np.sqrt(x**2 + y**2)\n",
    "\n",
    "    # Calculate the angle of the hit\n",
    "    hit_angle = np.degrees(np.arctan2(x, y))\n",
    "\n",
    "    # Find the corresponding boundary distance for this angle\n",
    "    if -45 <= hit_angle <= 45:\n",
    "        # Find the closest point in the boundary array\n",
    "        idx = np.abs(np.linspace(-45, 45, 100) - hit_angle).argmin()\n",
    "        boundary_distance = np.sqrt(boundary_x[idx]**2 + boundary_y[idx]**2)\n",
    "        return distance <= boundary_distance\n",
    "\n",
    "    return False\n",
    "\n",
    "def determine_catchable_home_run(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"Determining catchable home runs...\")\n",
    "    def is_catchable(row):\n",
    "        if row['is_home_run'] and row['exit_speed'] < 110 and row['hit_spin_rate'] < 3500 and is_within_field_boundaries(row):\n",
    "            return True\n",
    "        return False\n",
    "    df['is_catchable_home_run'] = df.apply(is_catchable, axis=1)\n",
    "    if debug:\n",
    "        print(\"Number of catchable home runs:\", df['is_catchable_home_run'].sum())\n",
    "        print(df[df['is_catchable_home_run'] == True][['hit_direction', 'adjusted_distance', 'landing_x_adjusted', 'landing_y_adjusted']].head())\n",
    "    return df\n",
    "\n",
    "def categorize_hit_direction(df, debug=False):\n",
    "    \"\"\"\n",
    "    Categorize hits into 'Left', 'Center', 'Right' based on 'horz_exit_angle'.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"Categorizing hit directions...\")\n",
    "    \n",
    "    conditions = [\n",
    "        df['horz_exit_angle'] < -15,\n",
    "        df['horz_exit_angle'] > 15\n",
    "    ]\n",
    "    choices = ['Left', 'Right']\n",
    "    df['hit_direction'] = np.select(conditions, choices, default='Center')\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Hit directions assigned:\", df['hit_direction'].unique())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_features(df, selected_features, include_target=False, target_variable='is_airout', debug=False):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame to only include columns specified in selected_features,\n",
    "    plus the target variable if include_target is True.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Original columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Prepare the list of columns to keep\n",
    "    columns_to_keep = selected_features.copy()\n",
    "    if include_target and target_variable in df.columns:\n",
    "        columns_to_keep.append(target_variable)\n",
    "    \n",
    "    # Filter columns\n",
    "    df_filtered = df[columns_to_keep].copy()\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Filtered columns: {df_filtered.columns.tolist()}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def visualize_hits(df, title='Hit Landing Points'):\n",
    "    \"\"\"\n",
    "    Visualize the landing points of hits with different categories.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='landing_x_adjusted', y='landing_y_adjusted', hue='hit_direction', style='is_home_run', data=df, palette='deep')\n",
    "    plt.axhline(0, color='black', linewidth=1)  # Baseline for y-axis\n",
    "    plt.axvline(0, color='black', linewidth=1)  # Baseline for x-axis\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Landing X (ft)')\n",
    "    plt.ylabel('Landing Y (ft)')\n",
    "    plt.legend(title='Hit Direction / Home Run')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Show catchable home runs\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='landing_x_adjusted', y='landing_y_adjusted', hue='is_catchable_home_run', data=df, palette={True: 'green', False: 'red'})\n",
    "    plt.title(f'{title} - Catchable Home Runs')\n",
    "    plt.xlabel('Landing X (ft)')\n",
    "    plt.ylabel('Landing Y (ft)')\n",
    "    plt.legend(title='Catchable Home Run')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Updated feature engineering pipeline to include pre-filtering visualizations\n",
    "def feature_engineering_pipeline(df, selected_features=None, include_target=False, debug=False):\n",
    "    \"\"\"\n",
    "    Run all feature engineering functions and filter for selected features if provided.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Initial columns: {df.columns.tolist()}\")\n",
    "        \n",
    "    global global_boundary  # Use global boundary to ensure consistency\n",
    "    if global_boundary is None:\n",
    "        global_boundary = simulate_outfield_boundary(average_distances, debug=debug)  # Compute once if not set\n",
    "    df = categorize_inning(df, debug)\n",
    "    df = create_count_scenario(df, debug)\n",
    "    df = transform_gamedate(df, debug)\n",
    "    df = categorize_temperature(df, debug)\n",
    "    df = calculate_physics_features(df, debug)\n",
    "    df = categorize_hit_direction(df, debug)\n",
    "    df = determine_home_run(df, boundary=None, debug=debug)\n",
    "    df = determine_catchable_home_run(df, debug=debug)\n",
    "\n",
    "    # Create 'is_foul' before the first visualization\n",
    "    if debug:\n",
    "        print(\"Determining foul balls...\")\n",
    "    df['is_foul'] = ~df.apply(lambda row: is_within_field_boundaries(row, boundary=global_boundary), axis=1)\n",
    "\n",
    "    # Visualize before filtering\n",
    "    print(\"Visualizing hits before applying any filters...\")\n",
    "    visualize_hits_with_field_boundary(df, title='All Hit Landing Points Before Filtering', hits_type='all', debug=debug)\n",
    "\n",
    "    print(\"Visualizing home runs before filtering...\")\n",
    "    visualize_hits_with_field_boundary(df[df['is_home_run']], title='Home Runs Before Filtering', hits_type='home_runs', debug=debug)\n",
    "\n",
    "    print(\"Visualizing foul balls before filtering...\")\n",
    "    visualize_hits_with_field_boundary(df[df['is_foul']], title='Foul Balls Before Filtering', hits_type='foul_balls', debug=debug)\n",
    "\n",
    "    # Apply filtering for non-catchable and foul balls\n",
    "    df = filter_non_catchable_and_foul(df, debug)\n",
    "\n",
    "    # Visualize after filtering\n",
    "    print(\"Visualizing hits after filtering uncatchable balls...\")\n",
    "    visualize_hits_with_field_boundary(df, title='All Hit Landing Points After Filtering', hits_type='all', debug=debug)\n",
    "\n",
    "    print(\"Visualizing home runs after filtering...\")\n",
    "    visualize_hits_with_field_boundary(df[df['is_home_run']], title='Home Runs After Filtering', hits_type='home_runs', debug=debug)\n",
    "\n",
    "    print(\"Visualizing foul balls after filtering...\")\n",
    "    visualize_hits_with_field_boundary(df[df['is_foul']], title='Foul Balls After Filtering', hits_type='foul_balls', debug=debug)\n",
    "\n",
    "    # Drop intermediate columns if necessary\n",
    "    df = df.drop(columns=['vert_angle_rad'], errors='ignore')\n",
    "\n",
    "    # Filter the DataFrame for selected features\n",
    "    if selected_features:\n",
    "        df = filter_features(df, selected_features, include_target=include_target, debug=debug)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Updated filter_non_catchable_and_foul function to clarify debug prints and ensure visibility\n",
    "def filter_non_catchable_and_foul(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"Filtering non-catchable and foul balls...\")\n",
    "        total_rows_before = df.shape[0]\n",
    "        \n",
    "    # Determine if each hit is a foul ball or not within field boundaries\n",
    "    df['is_foul'] = ~df.apply(is_within_field_boundaries, axis=1)\n",
    "    \n",
    "    # Filter to keep only catchable home runs or hits that are not fouls (remain in the park)\n",
    "    df_filtered = df[df['is_catchable_home_run'] | ~df['is_foul']]\n",
    "    \n",
    "    if debug:\n",
    "        total_rows_after = df_filtered.shape[0]\n",
    "        print(f\"Rows before filtering: {total_rows_before}\")\n",
    "        print(f\"Rows after filtering: {total_rows_after}\")\n",
    "        print(f\"Number of rows filtered out: {total_rows_before - total_rows_after}\")\n",
    "        print(\"Filtered out rows (foul balls outside park):\")\n",
    "        print(df[df['is_foul'] & ~df['is_catchable_home_run']][['hit_direction', 'adjusted_distance', 'landing_x_adjusted', 'landing_y_adjusted']].head())\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Run the updated pipeline\n",
    "if __name__ == '__main__':\n",
    "    import pandas as pd\n",
    "    # from data_loading import load_data\n",
    "    # from preprocessing import clean_data, handle_missing_values\n",
    "    \n",
    "    train_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-train.csv'\n",
    "    test_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-test.csv'\n",
    "    dict_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-dictionary.csv'\n",
    "\n",
    "    # Load data with debug mode enabled\n",
    "    train_df, test_df, data_dict_df = load_data(train_path, test_path, dict_path, debug=True)\n",
    "\n",
    "    # Preprocess data\n",
    "    train_df = clean_data(train_df, debug=True)\n",
    "    train_df = handle_missing_values(train_df, debug=True)\n",
    "\n",
    "    # Apply feature engineering with debug mode enabled\n",
    "    train_df = feature_engineering_pipeline(train_df, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/mariners_interview/defensive_feature_analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/mariners_interview/defensive_feature_analysis.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "GRAVITY = 32.174  # Gravity constant in ft/s^2\n",
    "\n",
    "# Function to calculate hang time based on exit speed and vertical angle\n",
    "def calculate_hang_time(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"\\nCalculating hang time for each hit...\")\n",
    "    \n",
    "    # Convert vertical exit angle to radians for trigonometric calculations\n",
    "    df['vert_angle_rad'] = np.radians(df['vert_exit_angle'])\n",
    "    \n",
    "    # Hang time calculation using kinematic equation: t = (2 * exit_speed * sin(vert_exit_angle)) / GRAVITY\n",
    "    df['hang_time'] = (2 * df['exit_speed'] * np.sin(df['vert_angle_rad'])) / GRAVITY\n",
    "    \n",
    "    # Ensure no negative or zero hang times\n",
    "    df['hang_time'] = df['hang_time'].clip(lower=0)\n",
    "\n",
    "    if debug:\n",
    "        # Show the first few hang time calculations\n",
    "        print(\"Hang time calculated for the first few records:\\n\", df[['hang_time', 'exit_speed', 'vert_exit_angle']].head())\n",
    "    return df\n",
    "\n",
    "# Function to estimate distance covered by the fielder based on their starting position\n",
    "def calculate_distance_covered(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"\\nEstimating distance covered by fielder...\")\n",
    "\n",
    "    # Define starting positions for each outfielder based on standard field layout\n",
    "    lf_start_x, lf_start_y = -90, 250  # Left Fielder starting position\n",
    "    cf_start_x, cf_start_y = 0, 350    # Center Fielder starting position\n",
    "    rf_start_x, rf_start_y = 90, 250   # Right Fielder starting position\n",
    "\n",
    "    # Function to calculate the distance from the starting position to the landing position\n",
    "    def get_distance_covered(row):\n",
    "        # Select the starting position based on fielder role\n",
    "        if row['first_fielder'] == row['lf_id']:\n",
    "            start_x, start_y = lf_start_x, lf_start_y\n",
    "        elif row['first_fielder'] == row['cf_id']:\n",
    "            start_x, start_y = cf_start_x, cf_start_y\n",
    "        elif row['first_fielder'] == row['rf_id']:\n",
    "            start_x, start_y = rf_start_x, rf_start_y\n",
    "        else:\n",
    "            return np.nan  # Return NaN if no matching fielder found\n",
    "        \n",
    "        # Calculate distance covered from starting position to landing position\n",
    "        distance = np.sqrt((row['landing_x_adjusted'] - start_x) ** 2 + (row['landing_y_adjusted'] - start_y) ** 2)\n",
    "        return distance\n",
    "\n",
    "    # Apply distance calculation to each row\n",
    "    df['distance_covered'] = df.apply(get_distance_covered, axis=1)\n",
    "\n",
    "    if debug:\n",
    "        # Display intermediate distances and validate with initial starting positions\n",
    "        print(\"Distance covered calculated for the first few records:\\n\", df[['distance_covered', 'landing_x_adjusted', 'landing_y_adjusted']].head())\n",
    "    return df\n",
    "\n",
    "# Function to estimate fielder reaction speed\n",
    "def calculate_reaction_speed(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"\\nCalculating fielder reaction speed...\")\n",
    "\n",
    "    # Reaction speed is calculated as distance covered divided by hang time\n",
    "    df['reaction_speed'] = df['distance_covered'] / df['hang_time']\n",
    "    \n",
    "    # Replace infinite or NaN values with zero for reaction speed\n",
    "    df['reaction_speed'] = df['reaction_speed'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    if debug:\n",
    "        # Validate the reaction speed for the first few records\n",
    "        print(\"Reaction speed calculated for the first few records:\\n\", df[['reaction_speed', 'distance_covered', 'hang_time']].head())\n",
    "    return df\n",
    "\n",
    "# Function to calculate estimated catch probability\n",
    "def calculate_catch_probability(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"\\nEstimating catch probability...\")\n",
    "\n",
    "    # Estimate catch probability using an exponential decay model based on distance covered and hang time\n",
    "    df['catch_probability'] = np.exp(-0.05 * df['distance_covered']) * (df['hang_time'] / 5)\n",
    "\n",
    "    # Clip catch probability to be between 0 and 1\n",
    "    df['catch_probability'] = np.clip(df['catch_probability'], 0, 1)\n",
    "\n",
    "    if debug:\n",
    "        # Display catch probability and related calculations\n",
    "        print(\"Catch probability estimated for the first few records:\\n\", df[['catch_probability', 'distance_covered', 'hang_time']].head())\n",
    "    return df\n",
    "\n",
    "# Function to categorize catch difficulty based on distance and hang time\n",
    "def categorize_catch_difficulty(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"\\nCategorizing catch difficulty...\")\n",
    "\n",
    "    # Step 1: Initialize catch_difficulty to 'Not_Caught' where is_airout is 0\n",
    "    df['catch_difficulty'] = np.where(df['is_airout'] == 0, 'Not_Caught', 'Unknown')\n",
    "\n",
    "    # Step 2: Define categorization conditions based on available metrics for other rows\n",
    "    conditions = [\n",
    "        (df['distance_covered'] < 50) & (df['hang_time'] > 4),             # Short distance, high hang time = Easy catch\n",
    "        (df['distance_covered'] < 100) & (df['hang_time'] > 3),            # Medium distance, moderate hang time = Moderate catch\n",
    "        (df['distance_covered'] >= 100) & (df['hang_time'] < 3),           # Long distance, low hang time = Difficult catch\n",
    "        (df['distance_covered'] >= 150) | (df['hang_time'] < 2)            # Very long distance or very low hang time = Very Difficult catch\n",
    "    ]\n",
    "    choices = ['Easy', 'Moderate', 'Difficult', 'Very Difficult']\n",
    "\n",
    "    # Step 3: Calculate catch difficulty for all rows using np.select\n",
    "    df['temp_catch_difficulty'] = np.select(conditions, choices, default='Uncatchable')\n",
    "\n",
    "    # Step 4: Overwrite catch_difficulty only for rows where valid metrics exist\n",
    "    mask_valid_metrics = ~df['first_fielder'].isna() | (~df['distance_covered'].isna() & ~df['hang_time'].isna())\n",
    "    df.loc[mask_valid_metrics, 'catch_difficulty'] = df.loc[mask_valid_metrics, 'temp_catch_difficulty']\n",
    "\n",
    "    # Drop the temporary column used for calculation\n",
    "    df.drop(columns=['temp_catch_difficulty'], inplace=True)\n",
    "\n",
    "    if debug:\n",
    "        # Show categorized difficulty for initial records\n",
    "        print(\"Catch difficulty categorized:\\n\", df[['catch_difficulty', 'distance_covered', 'hang_time', 'first_fielder', 'is_airout']].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to determine the position of the fielder who attempted the catch\n",
    "def determine_fielder_position(row):\n",
    "    if row['first_fielder'] == row['lf_id']:\n",
    "        return 'LF'\n",
    "    elif row['first_fielder'] == row['cf_id']:\n",
    "        return 'CF'\n",
    "    elif row['first_fielder'] == row['rf_id']:\n",
    "        return 'RF'\n",
    "    return 'Unknown'  # Catch not made or missing fielder ID information\n",
    "\n",
    "# Function to get count of unique values for catch difficulty\n",
    "def get_catch_difficulty_count(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"\\nGetting catch difficulty counts...\")\n",
    "    \n",
    "    # Get counts for each unique value in the catch_difficulty column\n",
    "    catch_difficulty_counts = df['catch_difficulty'].value_counts()\n",
    "    if debug:\n",
    "        print(\"Catch difficulty counts:\\n\", catch_difficulty_counts)\n",
    "    return catch_difficulty_counts\n",
    "\n",
    "# Extend the existing feature engineering pipeline with these new metrics\n",
    "def feature_engineering_with_defensive_metrics(df, selected_features=None, include_target=False, debug=False):\n",
    "\n",
    "    # Calculate new metrics for defensive evaluation\n",
    "    df = calculate_hang_time(df, debug)\n",
    "    df = calculate_distance_covered(df, debug)\n",
    "    df = calculate_reaction_speed(df, debug)\n",
    "    df = calculate_catch_probability(df, debug)\n",
    "    df = categorize_catch_difficulty(df, debug)\n",
    "    df['fielder_position'] = df.apply(determine_fielder_position, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example use of the pipeline\n",
    "if __name__ == '__main__':\n",
    "    # Assuming train_df is already loaded and preprocessed\n",
    "    train_df = feature_engineering_with_defensive_metrics(train_df, debug=True)\n",
    "    print(\"train_df columns =\", train_df.columns)\n",
    "    \n",
    "    # Get and print catch difficulty counts\n",
    "    catch_difficulty_counts = get_catch_difficulty_count(train_df, debug=True)\n",
    "\n",
    "\n",
    "# defensive metrics cause data leakage, great for post prediction analsis on these players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/mariners_interview/cluster_analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/mariners_interview/cluster_analysis.py\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Define a function to create a preprocessor for numeric features\n",
    "def get_numeric_preprocessor(numeric_features, debug=False):\n",
    "    \"\"\"\n",
    "    Create a preprocessor for handling numerical features.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"Creating numeric preprocessor...\")\n",
    "    \n",
    "    # Define a pipeline for numeric features with constant imputation and scaling\n",
    "    numeric_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),  # Set missing values to zero\n",
    "        ('scaler', StandardScaler())  # Scale numerical features\n",
    "    ])\n",
    "    \n",
    "    # Combine pipelines into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_pipeline, numeric_features)\n",
    "    ])\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Numeric preprocessing pipeline created for features: {numeric_features}\")\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "# Function to perform clustering and return the labeled dataset\n",
    "def perform_clustering(data, defensive_features, n_clusters=3, debug=False):\n",
    "    \"\"\"\n",
    "    Perform K-Means and DBSCAN clustering, add cluster labels to the dataset, and return the updated dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The preprocessed DataFrame containing defensive features.\n",
    "    - defensive_features: List of defensive features to use for clustering.\n",
    "    - n_clusters: Number of clusters for K-Means.\n",
    "    - debug: If True, display detailed outputs, visualizations, and analytics.\n",
    "    \n",
    "    Returns:\n",
    "    - Updated DataFrame with cluster labels.\n",
    "    \"\"\"\n",
    "    # Preprocess the defensive features\n",
    "    preprocessor = get_numeric_preprocessor(numeric_features=defensive_features, debug=debug)\n",
    "    X_preprocessed = preprocessor.fit_transform(data[defensive_features])\n",
    "\n",
    "    # Perform K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    data['defensive_cluster_kmeans'] = kmeans.fit_predict(X_preprocessed)\n",
    "\n",
    "    # Perform DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    data['defensive_cluster_dbscan'] = dbscan.fit_predict(X_preprocessed)\n",
    "\n",
    "    # Assign descriptive labels to K-Means clusters based on analysis\n",
    "    cluster_labels = {0: 'Quick Reactors', 1: 'Moderate Defenders', 2: 'Late Reactors'}\n",
    "    data['defensive_cluster_label'] = data['defensive_cluster_kmeans'].map(cluster_labels)\n",
    "\n",
    "    # Calculate detailed cluster statistics for K-Means clusters\n",
    "    cluster_summary = data.groupby('defensive_cluster_kmeans')[defensive_features].agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "\n",
    "    if debug:\n",
    "        # Print cluster statistics\n",
    "        print(\"Detailed K-Means Cluster Summary:\\n\", cluster_summary)\n",
    "\n",
    "        # Visualize K-Means Clusters\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=X_preprocessed[:, 0], y=X_preprocessed[:, 1], hue=data['defensive_cluster_kmeans'], palette='viridis')\n",
    "        plt.title('K-Means Clusters based on Defensive Stats')\n",
    "        plt.show()\n",
    "\n",
    "        # Visualize DBSCAN Clusters\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=X_preprocessed[:, 0], y=X_preprocessed[:, 1], hue=data['defensive_cluster_dbscan'], palette='coolwarm')\n",
    "        plt.title('DBSCAN Clusters based on Defensive Stats')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot distributions of key features across clusters\n",
    "        for feature in defensive_features:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.boxplot(x='defensive_cluster_kmeans', y=feature, data=data, palette='Set2')\n",
    "            plt.title(f'Distribution of {feature} Across K-Means Clusters')\n",
    "            plt.show()\n",
    "\n",
    "        # Pairplot to visualize pairwise relationships between features for each cluster\n",
    "        sns.pairplot(data, hue='defensive_cluster_kmeans', vars=defensive_features, palette='viridis')\n",
    "        plt.suptitle('Pairplot of Defensive Features by K-Means Clusters', y=1.02)\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate inter-cluster distances for K-Means clusters\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        distances = cdist(X_preprocessed, cluster_centers, 'euclidean')\n",
    "        data['distance_to_cluster_center'] = distances.min(axis=1)\n",
    "        print(\"Average distance to cluster centers for each cluster:\\n\", data.groupby('defensive_cluster_kmeans')['distance_to_cluster_center'].mean())\n",
    "\n",
    "        # Visualize the distribution of distances to cluster centers\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x='defensive_cluster_kmeans', y='distance_to_cluster_center', data=data, palette='Set3')\n",
    "        plt.title('Distance to Cluster Centers by K-Means Cluster')\n",
    "        plt.show()\n",
    "\n",
    "        # Visualize average values of each feature for each cluster using bar plots\n",
    "        cluster_mean_values = data.groupby('defensive_cluster_kmeans')[defensive_features].mean().reset_index()\n",
    "        cluster_mean_values_melted = cluster_mean_values.melt(id_vars='defensive_cluster_kmeans', var_name='Feature', value_name='Mean Value')\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='defensive_cluster_kmeans', y='Mean Value', hue='Feature', data=cluster_mean_values_melted, palette='tab10')\n",
    "        plt.title('Mean Value of Defensive Features for Each K-Means Cluster')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    return data\n",
    "\n",
    "# Main function to run clustering analysis and return labeled dataset\n",
    "def feature_engineering_with_cluster_analysis(df, debug=False):\n",
    "    \"\"\"\n",
    "    Main function to load data, perform clustering, and return the labeled dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input DataFrame with necessary features.\n",
    "    - debug: If True, enable debug mode and output additional analytics and visualizations.\n",
    "    \n",
    "    Returns:\n",
    "    - Labeled DataFrame with cluster labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select only the defensive-related stats for clustering\n",
    "    defensive_features = ['reaction_speed', 'distance_covered', 'catch_probability']\n",
    "\n",
    "    # Perform clustering and get the labeled data\n",
    "    df = perform_clustering(df, defensive_features, n_clusters=3, debug=debug)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load preprocessed data (adjust the file path as needed)\n",
    "    file_path = \"../../data/Seattle Mariners 2025 Analytics Internship/data-train-preprocessed.csv\"\n",
    "    \n",
    "    # Run clustering with debug mode enabled to visualize outputs\n",
    "    labeled_df = feature_engineering_with_cluster_analysis(pd.read_csv(file_path), debug=True)\n",
    "    print(\"Labeled DataFrame Head:\\n\", labeled_df.head())\n",
    "    print(\"Labeled DataFrame columns:\\n\", labeled_df.columns)\n",
    "\n",
    "# Analyze the clusters to find meaningful labels\n",
    "# For example:\n",
    "# Cluster 0: High Reaction Speed and Distance Covered = \"Quick Reactors\"\n",
    "# Cluster 1: Moderate Reaction Speed and Low Distance = \"Moderate Defenders\"\n",
    "# Cluster 2: Low Reaction Speed and High Distance = \"Late Reactors\"\n",
    "\n",
    "# clusters cause data leakage, great for post prediction analsis on these players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. modeling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/mariners_interview/training_and_eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/mariners_interview/training_and_eval.py\n",
    "\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def get_preprocessor(categorical_features, numeric_features, log_features, debug=False):\n",
    "    \"\"\"\n",
    "    Create a preprocessor for the pipeline and return selected features.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"Creating preprocessor...\")\n",
    "    \n",
    "    # Define pipelines for different feature types\n",
    "    categorical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    numeric_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    log_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('log_transform', FunctionTransformer(np.log1p)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Combine pipelines\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('num', numeric_pipeline, numeric_features),\n",
    "        ('log', log_pipeline, log_features)\n",
    "    ])\n",
    "    \n",
    "    # Collect selected features for each transformer\n",
    "    selected_features = categorical_features + numeric_features + log_features\n",
    "\n",
    "    if debug:\n",
    "        print(\"Selected features for preprocessing:\", selected_features)\n",
    "    \n",
    "    return preprocessor, selected_features\n",
    "\n",
    "def train_model(X_train, y_train, model, preprocessor, debug=False):\n",
    "    \"\"\"\n",
    "    Train a model with preprocessing.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"Training model:\", model)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Model trained.\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def grid_search_tuning(model, param_grid, X_train, y_train, preprocessor, scoring='roc_auc', cv=5, debug=False):\n",
    "    \"\"\"\n",
    "    Perform grid search tuning to find the best hyperparameters.\n",
    "\n",
    "    Args:\n",
    "    - model: The model to tune.\n",
    "    - param_grid: Dictionary of hyperparameters to search over.\n",
    "    - X_train: Training features.\n",
    "    - y_train: Training target variable.\n",
    "    - preprocessor: Preprocessing pipeline.\n",
    "    - scoring: Metric to evaluate the best parameters.\n",
    "    - cv: Number of cross-validation folds.\n",
    "    - debug: If True, print additional debug information.\n",
    "\n",
    "    Returns:\n",
    "    - Best fitted pipeline.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Starting grid search for {model} with params: {param_grid}\")\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Run GridSearchCV with the provided parameters\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, scoring=scoring, cv=cv, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Best params: {grid_search.best_params_}\")\n",
    "        print(f\"Best score: {grid_search.best_score_}\")\n",
    "\n",
    "    # Return the best fitted pipeline model\n",
    "    best_fitted_pipeline = grid_search.best_estimator_\n",
    "\n",
    "    return best_fitted_pipeline\n",
    "\n",
    "\n",
    "def save_best_params_from_pipeline(pipeline, filename, debug=False):\n",
    "    \"\"\"\n",
    "    Save the best parameters from a fitted pipeline's classifier.\n",
    "    \n",
    "    Args:\n",
    "    - pipeline: Fitted pipeline model.\n",
    "    - filename: Path to save the parameters.\n",
    "    - debug: If True, print debug information.\n",
    "    \"\"\"\n",
    "    # Extract the classifier parameters\n",
    "    best_params = pipeline.named_steps['classifier'].get_params()\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(best_params, f)\n",
    "    if debug:\n",
    "        print(f\"Best parameters saved to {filename}\")\n",
    "\n",
    "\n",
    "def load_best_params(filename, debug=False):\n",
    "    \"\"\"\n",
    "    Load the best hyperparameters from a file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    if debug:\n",
    "        print(f\"Best hyperparameters loaded from {filename}\")\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def save_model(model, filename, debug=False):\n",
    "    \"\"\"\n",
    "    Save the trained model to a file using joblib.\n",
    "    \"\"\"\n",
    "    joblib.dump(model, filename)\n",
    "    if debug:\n",
    "        print(f\"Model saved to {filename}\")\n",
    "\n",
    "def load_model(filename, debug=False):\n",
    "    \"\"\"\n",
    "    Load a trained model from a file using joblib.\n",
    "    \"\"\"\n",
    "    model = joblib.load(filename)\n",
    "    if debug:\n",
    "        print(f\"Model loaded from {filename}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, debug=False):\n",
    "    \"\"\"\n",
    "    Evaluate the model and return metrics.\n",
    "    \n",
    "    Args:\n",
    "    - model: The trained model.\n",
    "    - X_test: Test features.\n",
    "    - y_test: True labels for the test set.\n",
    "    - debug: If True, prints debug information.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"Evaluating model...\")\n",
    "    \n",
    "    # Predict values\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]  # Probability of class 1 (is_airout)\n",
    "    \n",
    "    # Compute metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    log_loss_score = log_loss(y_test, y_proba)  # Compute log loss\n",
    "\n",
    "    # Additional metrics: confusion matrix, precision, recall, F1-score\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "    f1 = report['1']['f1-score']\n",
    "    \n",
    "    metrics = {\n",
    "        'classification_report': report,\n",
    "        'roc_auc': roc_auc,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'log_loss': log_loss_score  # Add log loss to metrics dictionary\n",
    "    }\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"Log Loss: {log_loss_score:.4f}\")  # Print log loss\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix using seaborn heatmap.\n",
    "    \n",
    "    Args:\n",
    "    - cm: Confusion matrix.\n",
    "    - title: Title of the plot.\n",
    "    - cmap: Colormap for the heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_preprocessed_data(df, output_path, debug=False):\n",
    "    \"\"\"\n",
    "    Save the preprocessed DataFrame to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "    - df: Preprocessed DataFrame.\n",
    "    - output_path: Path to save the CSV file.\n",
    "    - debug: If True, print debug information.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    if debug:\n",
    "        print(f\"Preprocessed data saved to {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Define paths\n",
    "    train_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-train.csv'\n",
    "    test_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-test.csv'\n",
    "    dict_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-dictionary.csv'\n",
    "    preprocessed_train_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-train-preprocessed.csv'\n",
    "    \n",
    "    # Load data\n",
    "    train_df, test_df, data_dict_df = load_data(train_path, test_path, dict_path, debug=True)\n",
    "    \n",
    "    # Preprocess and feature engineer data\n",
    "    train_df = clean_data(train_df)\n",
    "    train_df = handle_missing_values(train_df)\n",
    "    train_df = feature_engineering_pipeline(train_df)\n",
    "    train_df = feature_engineering_with_defensive_metrics(train_df)\n",
    "    train_df = feature_engineering_with_cluster_analysis(train_df)\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    save_preprocessed_data(train_df, preprocessed_train_path, debug=True)\n",
    "    \n",
    "    # Define features and target\n",
    "    target = 'is_airout'\n",
    "    features = train_df.columns.drop(target)\n",
    "    X = train_df[features]\n",
    "    y = train_df[target]\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(\"x train columns=\", X_train.columns)\n",
    "    # Define feature types and create a preprocessor\n",
    "    categorical_features = ['month', 'day_of_week', 'temperature_category', 'count_scenario',\n",
    "                            'hit_direction', 'venue_id']\n",
    "\n",
    "    numeric_features = ['vert_exit_angle', 'horz_exit_angle', 'adjusted_distance']\n",
    "    log_features = ['exit_speed', 'hit_spin_rate']\n",
    "    preprocessor, selected_features = get_preprocessor(categorical_features, numeric_features, log_features, debug=True)\n",
    "    \n",
    "    # Define models and their hyperparameter grids\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(random_state=42),\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        #'LightGBM': LGBMClassifier(random_state=42),\n",
    "        'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "        'NeuralNet': MLPClassifier(max_iter=500, random_state=42)\n",
    "    }\n",
    "    \n",
    "    param_grids = {\n",
    "        'RandomForest': {'classifier__n_estimators': [50, 100, 200], 'classifier__max_depth': [None, 10, 20]},\n",
    "        'GradientBoosting': {'classifier__learning_rate': [0.01, 0.1], 'classifier__n_estimators': [100, 200]},\n",
    "        'XGBoost': {'classifier__learning_rate': [0.01, 0.1], 'classifier__max_depth': [3, 5, 7]},\n",
    "        #'LightGBM': {'classifier__learning_rate': [0.01, 0.1], 'classifier__num_leaves': [31, 50, 100]},\n",
    "        'CatBoost': {'classifier__learning_rate': [0.01, 0.1], 'classifier__depth': [4, 6, 8]},\n",
    "        'NeuralNet': {'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)], 'classifier__activation': ['relu', 'tanh']}\n",
    "    }\n",
    "\n",
    "    # Train, tune, and evaluate each model using grid search\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nGrid Search Tuning for {name}...\")\n",
    "        if name in param_grids:  # Check if the model has a parameter grid\n",
    "            # Perform grid search and get the best fitted pipeline\n",
    "            best_pipeline = grid_search_tuning(model, param_grids[name], X_train, y_train, preprocessor, scoring='roc_auc', cv=5, debug=True)\n",
    "\n",
    "            # Evaluate the best pipeline on validation data\n",
    "            metrics = evaluate_model(best_pipeline, X_val, y_val, debug=True)\n",
    "\n",
    "            # Print evaluation metrics, including log loss\n",
    "            print(f\"Metrics for Best {name}:\")\n",
    "            print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "            print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "            print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "            print(f\"Log Loss: {metrics['log_loss']:.4f}\")  # Print log loss score\n",
    "\n",
    "            # Plot confusion matrix\n",
    "            plot_confusion_matrix(metrics['confusion_matrix'], title=f'Confusion Matrix for Best {name}')\n",
    "\n",
    "            # Save the best pipeline\n",
    "            save_model(best_pipeline, f'../../data/Seattle Mariners 2025 Analytics Internship/models/{name.lower()}_best.pkl', debug=True)\n",
    "\n",
    "            # Optionally save the best parameters\n",
    "            save_best_params_from_pipeline(best_pipeline, f'../../data/Seattle Mariners 2025 Analytics Internship/models/{name.lower()}_best_params.json', debug=True)\n",
    "        else:\n",
    "            print(f\"No hyperparameter grid defined for {name}. Skipping grid search.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "# CatBoost proved to be the best model for your problem due to its efficient handling of categorical features, robust generalization through ordered boosting, and its ability to balance complexity and overfitting. The chosen hyperparameters (depth=6 and learning_rate=0.1) further reinforced its ability to perform well on your data.\n",
    "\n",
    "# In terms of model comparison:\n",
    "\n",
    "#     CatBoost outperformed XGBoost and Gradient Boosting by a small but significant margin in both ROC AUC and other evaluation metrics.\n",
    "#     It should be your go-to choice for this dataset, especially given the nature of features and the data distribution.\n",
    "    \n",
    "# ROC AUC: 0.9386\n",
    "# Accuracy: 0.8596\n",
    "# Precision: 0.8573\n",
    "# Recall: 0.8632\n",
    "# F1 Score: 0.8602\n",
    "# **Log Loss: 0.3223** # Log loss is a critical metric for evaluating probabilistic predictions because it penalizes both overconfident incorrect predictions and underconfident correct predictions. A lower log loss score indicates that the model’s probability estimates are closer to the true labels. In this case, the log loss score of 0.3223 suggests that CatBoost produced reliable probability predictions for the air out probability, minimizing uncertainty and maximizing the model’s predictive power.\n",
    "# Confusion Matrix:\n",
    "# [[5499  999]\n",
    "#  [ 901 5610]]\n",
    "\n",
    "# Given Question 1, CatBoost is particularly well-suited for this task due to its ability to natively handle categorical features, avoid overfitting through ordered boosting, and produce highly interpretable models. The dataset includes categorical columns like bat_side, pitch_side, day_of_week, and temperature_category, as well as various continuous features such as exit_speed and vert_exit_angle. CatBoost's specialized handling of categorical features means that it can extract more meaningful relationships from these features without the need for extensive preprocessing, which other models like XGBoost might require.\n",
    "\n",
    "# Moreover, CatBoost's robustness against overfitting and its ability to generalize better to new data make it an ideal choice for predicting a nuanced outcome like air out probability. Given that this problem involves predicting the likelihood of a rare event, CatBoost's advanced regularization methods help maintain balance between bias and variance, resulting in superior performance compared to other boosting models. This helps ensure reliable predictions on test data, which is crucial for achieving a low log loss score and maximizing evaluation metrics like AUC and F1 score, as seen in the results.\n",
    "\n",
    "# Ultimately, CatBoost not only fits the specific needs of this dataset but also aligns well with the evaluation criteria (log loss and generalization performance), making it the optimal model for this type of baseball event prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. prediction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/mariners_interview/prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/mariners_interview/prediction.py\n",
    "\n",
    "def predict(model, X, debug=False):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained model.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"Making predictions...\")\n",
    "    return model.predict_proba(X)[:, 1]  # Probability of class 1\n",
    "\n",
    "def save_predictions(predictions, output_path, debug=False):\n",
    "    \"\"\"\n",
    "    Save predictions to a CSV file.\n",
    "    \"\"\"\n",
    "    predictions.to_csv(output_path, index=False)\n",
    "    if debug:\n",
    "        print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import pandas as pd\n",
    "    # from data_loading import load_data\n",
    "    # from preprocessing import clean_data\n",
    "    # from feature_engineering import feature_engineering_pipeline\n",
    "    # from training_and_eval import load_model\n",
    "    \n",
    "    train_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-train.csv'\n",
    "    test_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-test.csv'\n",
    "    dict_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-dictionary.csv'\n",
    "    \n",
    "    # Load data with debug mode enabled\n",
    "    train_df, test_df, data_dict_df = load_data(train_path, test_path, dict_path, debug=True)\n",
    "    test_df = clean_data(test_df)\n",
    "    test_df_original = test_df.copy() \n",
    "    test_df = feature_engineering_pipeline(test_df)\n",
    "    features = test_df.columns  # Assuming the same features as training\n",
    "    X_test = test_df[features]\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model('../../data/Seattle Mariners 2025 Analytics Internship/models/catboost_best.pkl')\n",
    "    \n",
    "    # Make predictions\n",
    "    test_df['p_airout'] = predict(model, X_test, debug=True)\n",
    "\n",
    "    # Combine predictions with original test data\n",
    "    test_df_original = test_df_original.drop(columns=['p_airout'])\n",
    "    test_df_combined = pd.concat([test_df_original.reset_index(drop=True), test_df[['p_airout']].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Save predictions\n",
    "    save_predictions(test_df_combined, '../../data/Seattle Mariners 2025 Analytics Internship/test_predictions.csv', debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/mariners_interview/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/mariners_interview/main.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from prediction import predict, save_predictions\n",
    "# from training_and_eval import (\n",
    "#     get_preprocessor, train_model, save_model, load_model, grid_search_tuning, evaluate_model, save_best_params_from_pipeline\n",
    "# )\n",
    "# from data_loading import load_data\n",
    "# from preprocessing import clean_data, handle_missing_values\n",
    "# from feature_engineering import feature_engineering_pipeline\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Paths to data files\n",
    "    train_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-train.csv'\n",
    "    test_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-test.csv'\n",
    "    dict_path = '../../data/Seattle Mariners 2025 Analytics Internship/data-dictionary.csv'\n",
    "    \n",
    "    # Load data\n",
    "    train_df, test_df, data_dict_df = load_data(train_path, test_path, dict_path, debug=True)\n",
    "    \n",
    "    # Preprocessing\n",
    "    train_df = clean_data(train_df, debug=True)\n",
    "    train_df = handle_missing_values(train_df, debug=True)\n",
    "    \n",
    "    # Define feature types for preprocessing\n",
    "    categorical_features = ['month', 'day_of_week', 'temperature_category', 'count_scenario', 'hit_direction']\n",
    "    numeric_features = ['vert_exit_angle', 'horz_exit_angle', 'adjusted_distance']\n",
    "    log_features = ['exit_speed', 'hit_spin_rate']\n",
    "    \n",
    "    # Get preprocessor and selected features\n",
    "    preprocessor, selected_features = get_preprocessor(categorical_features, numeric_features, log_features, debug=True)\n",
    "    \n",
    "    # Feature Engineering with filtering for train_df, include target variable\n",
    "    train_df = feature_engineering_pipeline(train_df, selected_features=selected_features, include_target=True, debug=True)\n",
    "    \n",
    "    # Define target and features\n",
    "    target = 'is_airout'\n",
    "    features = selected_features\n",
    "    X = train_df[features]\n",
    "    y = train_df[target]\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    best_model_pipeline = CatBoostClassifier(depth=6, learning_rate=0.1, random_state=42, verbose=0)\n",
    "    model_pipeline = train_model(X_train, y_train, best_model_pipeline, preprocessor, debug=True)\n",
    "    \n",
    "    metrics = evaluate_model(model_pipeline, X_val, y_val, debug=True)\n",
    "    print(f\"Metrics:\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    save_model(model_pipeline, f'../../data/Seattle Mariners 2025 Analytics Internship/models/catboost.pkl', debug=True)\n",
    "    \n",
    "    # Process test data\n",
    "    print(\"test_df columns = \", test_df.columns)\n",
    "    test_df = clean_data(test_df, debug=True)\n",
    "    test_df_original = test_df.copy()\n",
    "    test_df = feature_engineering_pipeline(test_df, selected_features=selected_features, include_target=False, debug=True)\n",
    "    X_test = test_df[features]\n",
    "    \n",
    "    # Make predictions\n",
    "    test_df['p_airout'] = predict(model_pipeline, X_test, debug=True)\n",
    "    \n",
    "    # Combine predictions with original test data\n",
    "    test_df_original = test_df_original.drop(columns=['p_airout'])\n",
    "    test_df_combined = pd.concat([test_df_original.reset_index(drop=True), test_df[['p_airout']].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Save predictions\n",
    "    save_predictions(test_df_combined, '../../data/Seattle Mariners 2025 Analytics Internship/test_predictions.csv', debug=True)\n",
    "\n",
    "    print(\"Predictions made successfully and saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App section for complete explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/mariners_data_streamlit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/mariners_data_streamlit.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the necessary modules\n",
    "from mariners_interview.training_and_eval import load_model\n",
    "from mariners_interview.prediction import predict\n",
    "from mariners_interview.feature_engineering import calculate_physics_features, calculate_hit_trajectory, visualize_hit_trajectory\n",
    "from mariners_interview.player_scouting_report import generate_scouting_report\n",
    "\n",
    "# Define constants\n",
    "MODEL_PATH = 'data/Seattle Mariners 2025 Analytics Internship/models/catboost_best.pkl'\n",
    "PREPROCESSED_DATA_PATH = 'data/Seattle Mariners 2025 Analytics Internship/data-train-preprocessed.csv'\n",
    "\n",
    "# Load the model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# List of selected features required by the model\n",
    "selected_features = [\n",
    "    'month',\n",
    "    'day_of_week',\n",
    "    'temperature_category',\n",
    "    'count_scenario',\n",
    "    'hit_direction',\n",
    "    'vert_exit_angle',\n",
    "    'horz_exit_angle',\n",
    "    'exit_speed',\n",
    "    'hit_spin_rate',\n",
    "    'adjusted_distance',\n",
    "    'venue_id'\n",
    "]\n",
    "\n",
    "def get_min_max_values(df, numeric_features):\n",
    "    min_max_values = {}\n",
    "    for feature in numeric_features:\n",
    "        min_value = df[feature].min()\n",
    "        max_value = df[feature].max()\n",
    "        min_max_values[feature] = (min_value, max_value)\n",
    "    return min_max_values\n",
    "\n",
    "# Section 1: Project Information\n",
    "def info_section():\n",
    "    st.header(\"Information on Process:\")\n",
    "    st.markdown(\"\"\"\n",
    "     \n",
    "\n",
    " **Question 1 Project Overview: \n",
    " \n",
    " Goal: 1. In the Google Drive folder linked below you will find three files: `data-train.csv`, `data-test.csv`,\n",
    "and `data-dictionary.csv’.\n",
    "We have provided the 2023 season of contacted balls for two Minor League levels, along with\n",
    "play metadata and Trackman hit tracking information, pre-filtered to non-infield plays. This\n",
    "includes foul balls and home runs not necessarily hit into playable territory. The `train.csv` file\n",
    "includes two additional columns: on balls caught for outs, `is_airout` will be 1 and `first_fielder`\n",
    "will give the player id responsible for the out. On balls not caught for outs, `is_airout` will be 0\n",
    "and `first_fielder` will be null.\n",
    "Your objective is to predict the air out probability for batted balls in the `data-test.csv` file and\n",
    "fill out the `p_airout` column in that .csv with your estimate. To assist you, we have included\n",
    "the ‘data-dictionary.csv’ file to explain what each column in the attached datasets represents.\n",
    "You may use whatever method or language you like, but you must submit the code you\n",
    "used to generate and evaluate your predictions. You will be evaluated on both the log loss\n",
    "score of your predictions and on your process in generating the predictions. Please also\n",
    "include a brief explanation of your process for an R&amp;D audience and what steps you would\n",
    "take to improve on this model if you had more resources.\n",
    "\n",
    "The goal of this project is to predict the probability (p_airout) that a batted ball results in an air out. This involves several steps:\n",
    "\n",
    " **Preprocessing and Cleaning**\n",
    " **Feature Engineering**\n",
    " **Exploratory Data Analysis (EDA)**\n",
    " **Model Selection and Training**\n",
    " **Evaluation**\n",
    " **Prediction and Analysis**\n",
    "\n",
    "# Project Summary: Minor Leage Outfielder Prediction of Airouts for the 2023 Season\n",
    "---\n",
    "\n",
    "## ** Preprocessing and Cleaning**\n",
    "\n",
    "### **Handling Missing Values**\n",
    "\n",
    "- **Spin Rate Filtering**: We noticed that the hit_spin_rate feature had only **1.42%** of its values present. This low percentage suggests that the data is mostly missing, possibly due to technical malfunctions during data collection. Therefore, we decided to drop this feature to prevent it from introducing noise into our model.\n",
    "\n",
    "  **Why?**\n",
    "\n",
    "  - Features with excessive missing values can negatively impact model performance.\n",
    "  - Imputing such a high percentage of missing data might introduce bias.\n",
    "\n",
    "### **Dropping Unnecessary Columns**\n",
    "\n",
    "- **Player IDs**: Columns like first_fielder, lf_id, cf_id, and rf_id were dropped. These identifiers are specific to individual players and do not contribute to the generalized prediction of p_airout.\n",
    "\n",
    "  **Why?**\n",
    "\n",
    "  - Removing irrelevant or high-cardinality features reduces complexity.\n",
    "  - Helps prevent overfitting to specific players in the training data.\n",
    "\n",
    "### **Handling Missing Rows**\n",
    "\n",
    "- **Dropping Rows with Missing hit_spin_rate**: After dropping the hit_spin_rate column, we ensured that any remaining rows with missing critical values were handled appropriately.\n",
    "\n",
    "  **Why?**\n",
    "\n",
    "  - Ensures data integrity for model training.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Feature Engineering**\n",
    "\n",
    "Feature engineering is crucial to enhance the predictive power of our models. Here's what we did:\n",
    "\n",
    "### **3.1 Recreating the Ballpark**\n",
    "\n",
    "- **Average Park Dimensions**: Since we lacked specific venue information for each play, we used the average dimensions of major league parks to create a standardized field.\n",
    "\n",
    "  **Why?**\n",
    "\n",
    "  - Provides a consistent basis to evaluate minor league players on a major league scale.\n",
    "  - Helps in determining whether a hit would be a home run or a foul ball.\n",
    "\n",
    "- **Simulating the Outfield Boundary**: Using the average distances, we simulated the outfield boundary to classify hits as home runs or in-play.\n",
    "\n",
    "  **Graphics Included**:\n",
    "\n",
    "  - **Ballpark Diagram with Hits**: Visualizations showing hit landing points with the outfield boundary.\n",
    "  - **Home Runs vs. Non-Home Runs**: Separate plots highlighting home runs and catchable balls.\n",
    "\n",
    "  **Why?**\n",
    "\n",
    "  - Visual aids help in understanding the spatial distribution of hits.\n",
    "  - Allows us to filter out uncatchable home runs and focus on plays where the fielder's actions matter.\n",
    "\n",
    "### **3.2 Estimating Landing Spots**\n",
    "\n",
    "- **Physics-Based Calculations**: We used the exit speed and angles to estimate the landing spot of the ball.\n",
    "\n",
    "  - **Adjusted Distance**: Calculated using projectile motion equations, adjusted for spin rate to account for aerodynamic effects.\n",
    "\n",
    "  **Why?**\n",
    "\n",
    "  - Provides a more accurate estimation of where the ball would land.\n",
    "  - Essential for determining if a ball is catchable.\n",
    "\n",
    "### **3.3 Categorizing Game Context Variables**\n",
    "\n",
    "- **Count Scenarios**: Combined pre_balls, pre_strikes, pre_outs, and inning into a single feature called count_scenario. This categorizes the game situation into different scenarios.\n",
    "\n",
    "  **Why?**\n",
    "\n",
    "  - Simplifies multiple related features into one, making it easier for the model to learn patterns.\n",
    "  - One-hot encoding of count_scenario allows the model to evaluate deeper on each scenario level.\n",
    "\n",
    "- **Temperature Categories**: We categorized the temperature into:\n",
    "\n",
    "  - **Cold**: Below 70°F\n",
    "  - **Moderate**: 70°F to 90°F\n",
    "  - **Hot**: Above 90°F\n",
    "\n",
    "  **Why?**\n",
    "\n",
    "  - Temperature can affect ball physics and player performance.\n",
    "  - Categorization helps the model understand the impact without overcomplicating.\n",
    "\n",
    "### **3.4 Identifying Foul Balls and Catchable Hits**\n",
    "\n",
    "- **In-Field Foul Balls**: We attempted to identify foul balls that remained in the park and were potentially catchable.\n",
    "\n",
    "  **Why?**\n",
    "\n",
    "  - To include plays where fielders have a chance to make an out.\n",
    "  - Helps in accurately modeling the is_airout outcome.\n",
    "\n",
    "### **3.5 Defensive Metrics (Post-Prediction Analysis)**\n",
    "\n",
    "- **Calculating Defensive Stats**:\n",
    "\n",
    "  - **Reaction Speed**: Estimated based on the distance covered by the fielder and the hang time of the ball.\n",
    "  - **Distance Covered**: Calculated using the estimated landing spot and assumed starting positions for fielders.\n",
    "  - **Catch Difficulty**: Categorized as 'Easy', 'Moderate', 'Difficult', or 'Very Difficult' based on hang time and distance.\n",
    "\n",
    "  **Why?**\n",
    "\n",
    "  - Provides insights into player performance.\n",
    "  - **Note**: These metrics were calculated for analysis **after** predictions to avoid data leakage.\n",
    "\n",
    "- **Cluster Analysis**:\n",
    "\n",
    "  - Performed clustering on defensive metrics to identify groups of players with similar defensive abilities.\n",
    "\n",
    "  **Why?**\n",
    "\n",
    "  - Helps in post-prediction analysis of players.\n",
    "  - Avoids introducing bias into the predictive model.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Exploratory Data Analysis (EDA)**\n",
    "\n",
    "### **Visualizations**\n",
    "\n",
    "- **Distribution of Adjusted Distances**: Histograms showing how far balls typically travel.\n",
    "\n",
    "- **Hit Landing Points**: Scatter plots of landing positions colored by hit direction and whether they resulted in an air out.\n",
    "\n",
    "- **Temperature Impact**: Analysis of how temperature categories affect hit outcomes.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "- EDA helps in understanding the data patterns.\n",
    "- Identifies potential features that could improve model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Model Selection and Training**\n",
    "\n",
    "We tested several classifiers:\n",
    "\n",
    "- **Random Forest**\n",
    "- **Gradient Boosting**\n",
    "- **XGBoost**\n",
    "- **Logistic Regression**\n",
    "- **CatBoost**\n",
    "\n",
    "### **Why CatBoost Was Chosen**\n",
    "\n",
    "- **Handling of Categorical Features**: CatBoost natively handles categorical variables without extensive preprocessing.\n",
    "\n",
    "- **Avoiding Overfitting**: Uses ordered boosting and other regularization techniques.\n",
    "\n",
    "- **Performance Metrics**: CatBoost outperformed other models in key metrics.\n",
    "\n",
    "  - **ROC AUC**: 0.9386\n",
    "  - **Accuracy**: 0.8596\n",
    "  - **Precision**: 0.8573\n",
    "  - **Recall**: 0.8632\n",
    "  - **F1 Score**: 0.8602\n",
    "  - **Log Loss**: 0.3223\n",
    "\n",
    "    **Explanation of Log Loss**:\n",
    "\n",
    "    - Log loss penalizes both overconfident incorrect predictions and underconfident correct predictions.\n",
    "    - A lower log loss indicates better probability estimates.\n",
    "    - CatBoost's log loss suggests reliable probabilistic predictions.\n",
    "\n",
    "**Conclusion**:\n",
    "\n",
    "- CatBoost's ability to handle our data's specific characteristics made it the optimal choice.\n",
    "- Its performance in both classification accuracy and probability estimation was superior.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Evaluation**\n",
    "\n",
    "### **Confusion Matrix**\n",
    "\n",
    "[[5499  999]\n",
    " [ 901 5610]]\n",
    "\n",
    "\n",
    "\n",
    "- **True Positives (TP)**: 5610\n",
    "- **True Negatives (TN)**: 5499\n",
    "- **False Positives (FP)**: 999\n",
    "- **False Negatives (FN)**: 901\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- The model correctly identified a high number of air outs and non-air outs.\n",
    "- The balance between precision and recall is acceptable for this context.\n",
    "\n",
    "### **Receiver Operating Characteristic (ROC) Curve**\n",
    "\n",
    "- A high ROC AUC score indicates good discrimination between the two classes.\n",
    "\n",
    "**Why Evaluation Matters**:\n",
    "\n",
    "- Ensures that the model generalizes well to unseen data.\n",
    "- Helps in understanding the trade-offs between different types of errors.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Prediction and Analysis**\n",
    "\n",
    "### **Making Predictions**\n",
    "\n",
    "- The model was applied to the test dataset to predict p_airout.\n",
    "- Predictions were concatenated back to the original test data for further analysis.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "- Allows for analysis of predictions in the context of the original features.\n",
    "- Enables us to study specific scenarios where the model performs well or needs improvement.\n",
    "\n",
    "### **Interactive Prediction Section**\n",
    "\n",
    "To provide an interactive experience, we included a section where users can input data points and receive a p_airout prediction.\n",
    "\n",
    "**Fields Required**:\n",
    "\n",
    "- **Month**: Numerical value (1-12)\n",
    "- **Day of Week**: String (e.g., \"Monday\")\n",
    "- **Temperature Category**: \"Cold\", \"Moderate\", \"Hot\"\n",
    "- **Count Scenario**: String combining counts (e.g., \"1-2-1-Mid\")\n",
    "- **Hit Direction**: \"Left\", \"Center\", \"Right\"\n",
    "- **Venue ID**: Numerical value\n",
    "- **Vertical Exit Angle**: Float\n",
    "- **Horizontal Exit Angle**: Float\n",
    "- **Adjusted Distance**: Float\n",
    "- **Exit Speed**: Float\n",
    "- **Hit Spin Rate**: Float\n",
    "\n",
    "**Example**:\n",
    "\n",
    "json\n",
    "{\n",
    "    \"month\": 6,\n",
    "    \"day_of_week\": \"Wednesday\",\n",
    "    \"temperature_category\": \"Moderate\",\n",
    "    \"count_scenario\": \"1-2-1-Mid\",\n",
    "    \"hit_direction\": \"Center\",\n",
    "    \"venue_id\": 10,\n",
    "    \"vert_exit_angle\": 35.0,\n",
    "    \"horz_exit_angle\": -5.0,\n",
    "    \"adjusted_distance\": 300.0,\n",
    "    \"exit_speed\": 90.0,\n",
    "    \"hit_spin_rate\": 2200.0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "**Predicted p_airout**:\n",
    "\n",
    "- The model outputs a probability between 0 and 1.\n",
    "- In this example, p_airout might be 0.85, indicating an 85% chance of the hit resulting in an air out.\n",
    "\n",
    "\n",
    "### **Analysis of Predictions**\n",
    "\n",
    "By analyzing the predictions:\n",
    "\n",
    "- **Identify Patterns**: See how different features impact the probability.\n",
    "- **Model Limitations**: Find scenarios where the model might not perform as well.\n",
    "- **Further Improvements**: Use insights to refine the model or feature engineering steps.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Deployment**\n",
    "\n",
    "### **Docker Environment**\n",
    "\n",
    "- **Modularized Setup**: Used Docker and Conda to create a reproducible environment.\n",
    "- **Automated Workflow**: Wrapped the entire process into a main function for ease of use.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "- Ensures consistency across different systems.\n",
    "- Simplifies deployment and scaling.\n",
    "\n",
    "### **Streamlit**\n",
    "\n",
    "- **Streamlit App**: Provides an interactive walkthrough of the entire project, including data visualizations and the prediction interface.\n",
    "\n",
    "  - **Features**:\n",
    "\n",
    "    - Step-by-step explanations.\n",
    "    - Graphics showcasing the ballpark and hit distributions.\n",
    "    - Input forms for user predictions.\n",
    "\n",
    "- **FastAPI App**: Serves the model predictions via an API endpoint.\n",
    "\n",
    "  - **Features**:\n",
    "\n",
    "    - Allows integration with other applications.\n",
    "    - Accepts input data in JSON format and returns predictions.\n",
    "\n",
    "## **Future Improvements: question 1 future improvements: \n",
    "# park factors to it to get more or less runs scored or just the air density to make it easier\n",
    "# venue_id information to the actual venue measurements for foul ball checks and home run catchability to be exact\n",
    "# add in game factors to get: putouts leading to fielding percentage\n",
    "# more granular data for ultimate zone rating and Defensive runs saved\n",
    "# exact outfielder positions at the time of hit so we could get actual reaction speeds vs accelerations\n",
    "# log loss and roc by class to discover which are most important\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "By meticulously explaining each step and decision, including visual aids and interactive elements, we provide a comprehensive understanding of the project. Users can not only see the final predictions but also grasp the underlying processes that led to them.\n",
    "\n",
    "If you have any questions or need further clarification on any section, feel free to ask!\")\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "def prediction_section():\n",
    "    st.header(\"Prediction Interface\")\n",
    "    st.markdown(\"### Provide input values for the prediction model:\")\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    preprocessed_df = pd.read_csv(PREPROCESSED_DATA_PATH)\n",
    "    \n",
    "    # Get min and max values for numeric features\n",
    "    numeric_features = ['vert_exit_angle', 'horz_exit_angle', 'exit_speed', 'hit_spin_rate']\n",
    "    min_max_values = get_min_max_values(preprocessed_df, numeric_features)\n",
    "    \n",
    "    # Get unique options for categorical features\n",
    "    count_scenario_options = sorted(preprocessed_df['count_scenario'].dropna().unique())\n",
    "    day_of_week_options = sorted(preprocessed_df['day_of_week'].dropna().unique())\n",
    "    temperature_category_options = sorted(preprocessed_df['temperature_category'].dropna().unique())\n",
    "    hit_direction_options = sorted(preprocessed_df['hit_direction'].dropna().unique())\n",
    "    \n",
    "    # Month min and max\n",
    "    month_min = int(preprocessed_df['month'].min())\n",
    "    month_max = int(preprocessed_df['month'].max())\n",
    "    \n",
    "    # Collect user inputs\n",
    "    month = st.slider(\"Month\", min_value=month_min, max_value=month_max, value=int(preprocessed_df['month'].median()))\n",
    "    day_of_week = st.selectbox(\"Day of Week\", options=day_of_week_options)\n",
    "    temperature_category = st.selectbox(\"Temperature Category\", options=temperature_category_options)\n",
    "    count_scenario = st.selectbox(\"Count Scenario\", options=count_scenario_options)\n",
    "    hit_direction = st.selectbox(\"Hit Direction\", options=hit_direction_options)\n",
    "    \n",
    "    vert_exit_angle_min, vert_exit_angle_max = min_max_values['vert_exit_angle']\n",
    "    vert_exit_angle = st.slider(\"Vertical Exit Angle (degrees)\", min_value=float(vert_exit_angle_min), max_value=float(vert_exit_angle_max), value=float(preprocessed_df['vert_exit_angle'].median()))\n",
    "    \n",
    "    horz_exit_angle_min, horz_exit_angle_max = min_max_values['horz_exit_angle']\n",
    "    horz_exit_angle = st.slider(\"Horizontal Exit Angle (degrees)\", min_value=float(horz_exit_angle_min), max_value=float(horz_exit_angle_max), value=float(preprocessed_df['horz_exit_angle'].median()))\n",
    "    \n",
    "    exit_speed_min, exit_speed_max = min_max_values['exit_speed']\n",
    "    exit_speed = st.slider(\"Exit Speed (mph)\", min_value=float(exit_speed_min), max_value=float(exit_speed_max), value=float(preprocessed_df['exit_speed'].median()))\n",
    "    \n",
    "    hit_spin_rate_min, hit_spin_rate_max = min_max_values['hit_spin_rate']\n",
    "    hit_spin_rate = st.slider(\"Hit Spin Rate (rpm)\", min_value=float(hit_spin_rate_min), max_value=float(hit_spin_rate_max), value=float(preprocessed_df['hit_spin_rate'].median()))\n",
    "    \n",
    "    # Compute adjusted_distance\n",
    "    GRAVITY = 32.174  # ft/s^2\n",
    "    vert_angle_rad = np.radians(vert_exit_angle)\n",
    "    estimated_distance = ((exit_speed ** 2) * np.sin(2 * vert_angle_rad)) / GRAVITY\n",
    "    adjusted_distance = estimated_distance * (1 + (hit_spin_rate / 15000))\n",
    "    \n",
    "    # Set venue_id to a sample value (most frequent venue_id)\n",
    "    venue_id = int(preprocessed_df['venue_id'].mode()[0])  # Use the most common venue_id\n",
    "    \n",
    "    # Create input DataFrame for prediction\n",
    "    input_data = pd.DataFrame({\n",
    "        \"month\": [month],\n",
    "        \"day_of_week\": [day_of_week],\n",
    "        \"temperature_category\": [temperature_category],\n",
    "        \"count_scenario\": [count_scenario],\n",
    "        \"hit_direction\": [hit_direction],\n",
    "        \"vert_exit_angle\": [vert_exit_angle],\n",
    "        \"horz_exit_angle\": [horz_exit_angle],\n",
    "        \"exit_speed\": [exit_speed],\n",
    "        \"hit_spin_rate\": [hit_spin_rate],\n",
    "        \"adjusted_distance\": [adjusted_distance],\n",
    "        \"venue_id\": [venue_id]\n",
    "    })\n",
    "    \n",
    "    # Use the selected features directly\n",
    "    X_input = input_data[selected_features]\n",
    "    \n",
    "    # Make prediction\n",
    "    if st.button(\"Predict\"):\n",
    "        probability_airout = predict(model, X_input, debug=False)[0]\n",
    "        st.write(f\"### Probability of Air Out by Outfielder: **{probability_airout:.2f}**\")\n",
    "\n",
    "        # Compute physics features for visualization\n",
    "        input_data = calculate_physics_features(input_data)\n",
    "\n",
    "        # Visualize the hit trajectory and landing point\n",
    "        st.write(\"#### Hit Trajectory and Landing Visualization\")\n",
    "        fig = visualize_hit_trajectory(input_data)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "# Section 3: Scouting Report Generator\n",
    "def scouting_report_section():\n",
    "    st.header(\"Scouting Report Generator\")\n",
    "    st.markdown(\"\"\"\n",
    "                It is October 1 st , 2023, and with the conclusion of the Minor League season the Director of\n",
    "Player Development of the Seattle Mariners is interested in 15411’s outfield defense. Based on\n",
    "the data provided, and assuming all other things being equal, write a one-page report for a\n",
    "coaching audience breaking down this player’s defensive performance and abilities.\n",
    "\n",
    "\n",
    "# Answer:\n",
    "Scouting Report: Player ID 15411\n",
    "\n",
    "Player Overview:\n",
    "This report provides an in-depth analysis of the defensive performance of Player ID 15411, using advanced metrics and comparisons against the league averages and cluster groupings. Key metrics used include reaction speed, distance covered, and catch probability. Each metric is evaluated at an aggregate level and within specific game conditions.\n",
    "League Comparison Analysis:\n",
    "Metric\tPlayer Average\tLeague Average\tDifference\tPercent Difference\tPlayer Percentile\tLeague Percentile\n",
    "Reaction Speed\t13.65\t13.43\t0.22\t1.62%\t50.15\t50.00\n",
    "Distance Covered\t83.11\t82.38\t0.73\t0.89%\t50.15\t50.00\n",
    "Catch Probability\t0.061\t0.058\t0.003\t5.21%\t50.15\t50.00\n",
    "\n",
    "Performance Highlights:\n",
    "\n",
    "    Reaction Speed: The player's average reaction speed of 13.65 is slightly above the league average of 13.43, with a 1.62% difference, placing them in the 50th percentile for this metric.\n",
    "    Distance Covered: The player covers an average distance of 83.11, slightly higher than the league's average of 82.38, with a difference of 0.73. This results in a similar percentile ranking as reaction speed.\n",
    "    Catch Probability: The player's catch probability stands at 0.061, exceeding the league average of 0.058. This 5.21% increase signifies strong performance in difficult catch scenarios.\n",
    "\n",
    "Condition-Based Performance Analysis:\n",
    "\n",
    "1. Performance under Temperature Category: Moderate\n",
    "\n",
    "    Reaction Speed: Player Average: 14.19, League Average: 13.40, Difference: 0.79, Percent Difference: 5.89%\n",
    "    Distance Covered: Player Average: 82.46, League Average: 82.02, Difference: 0.44, Percent Difference: 0.54%\n",
    "    Catch Probability: Player Average: 0.06, League Average: 0.06, Difference: -0.00, Percent Difference: -0.56%\n",
    "\n",
    "2. Performance under Bat Side: Right\n",
    "\n",
    "    Reaction Speed: Player Average: 14.07, League Average: 13.40, Difference: 0.66, Percent Difference: 4.93%\n",
    "    Distance Covered: Player Average: 83.67, League Average: 82.07, Difference: 1.60, Percent Difference: 1.95%\n",
    "    Catch Probability: Player Average: 0.06, League Average: 0.06, Difference: 0.00, Percent Difference: 3.95%\n",
    "\n",
    "3. Performance under Pitch Side: Right\n",
    "\n",
    "    Reaction Speed: Player Average: 12.42, League Average: 13.34, Difference: -0.93, Percent Difference: -6.94%\n",
    "    Distance Covered: Player Average: 80.68, League Average: 82.09, Difference: -1.42, Percent Difference: -1.73%\n",
    "    Catch Probability: Player Average: 0.06, League Average: 0.06, Difference: 0.00, Percent Difference: 6.61%\n",
    "\n",
    "Top 5 Players by Metric Comparison\n",
    "Metric\tTop 5 Players\tAverage Score\n",
    "Reaction Speed\tPlayer 123, Player 456, Player 789, Player 321, Player 654\t15.34\n",
    "Distance Covered\tPlayer 234, Player 876, Player 543, Player 109, Player 345\t98.12\n",
    "Catch Probability\tPlayer 567, Player 432, Player 765, Player 890, Player 111\t0.075\n",
    "Better Options:\n",
    "\n",
    "The top 5 players listed in each category showcase better performance in the respective metrics compared to Player ID 15411. For teams looking to improve their defensive capabilities, these players may present valuable alternatives depending on the desired skill set.\n",
    "Key Takeaways:\n",
    "\n",
    "    Consistency Across Metrics: Player ID 15411 demonstrates consistent performance across different metrics when compared to league averages. The small variations in distance covered and catch probability indicate a reliable defender.\n",
    "    Condition-Based Insights: The player's performance varies significantly under different game conditions such as temperature and pitch side, highlighting areas for potential improvement.\n",
    "    Better Alternatives: While Player ID 15411 performs well, top performers in reaction speed, distance covered, and catch probability have been identified as stronger options for similar roles.\n",
    "\n",
    "# Future improvements: RISP inclusion in defenders situations, LLM rag bot I feed this report to in order to have an informed bot on the stats we need to have scouting reports and easy data analysis\n",
    "\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "    st.subheader(\"Scout others Easier in the Report:\")\n",
    "    # Allow user to input player ID\n",
    "    player_id = st.number_input(\"Enter Player ID:\", min_value=0, value=15411)\n",
    "\n",
    "    # Option to select metrics and conditions\n",
    "    metrics_to_compare = st.multiselect(\n",
    "        \"Select Metrics to Compare:\",\n",
    "        options=['reaction_speed', 'distance_covered', 'catch_probability'],\n",
    "        default=['reaction_speed', 'distance_covered', 'catch_probability']\n",
    "    )\n",
    "\n",
    "    condition_columns = st.multiselect(\n",
    "        \"Select Conditions to Analyze:\",\n",
    "        options=['temperature_category', 'bat_side', 'pitch_side', 'top', 'venue_id', 'count_scenario'],\n",
    "        default=['temperature_category', 'bat_side', 'pitch_side']\n",
    "    )\n",
    "\n",
    "    if st.button(\"Generate Report\"):\n",
    "        file_path = PREPROCESSED_DATA_PATH\n",
    "        report, league_comparison = generate_scouting_report(\n",
    "            file_path, player_id, metrics=metrics_to_compare,\n",
    "            condition_columns=condition_columns, debug=False\n",
    "        )\n",
    "        if report:\n",
    "            st.markdown(report)\n",
    "        else:\n",
    "            st.error(f\"No data found for player with ID {player_id}.\")\n",
    "\n",
    "\n",
    "# Section 3: Scouting Report Generator\n",
    "def mariners_improvements_section():\n",
    "    st.header(\"Mariners Improvements\")\n",
    "    st.markdown(\"\"\"\n",
    "                3. In approximately 300 words, what is a recent mistake the Mariners organization has made, and\n",
    "why do you consider it a mistake?\n",
    "Recent Mistake by the Mariners Organization: Pitching Strategy and Roster Management\n",
    "\n",
    "One recent mistake by the Mariners organization has been their reluctance to adopt a more flexible pitching strategy to address late-game performance issues. The team’s away record underscores this concern, with several close losses stemming from bullpen struggles. Despite having a strong starting rotation featuring arms like Castillo, Gilbert, and Kirby, the bullpen’s inconsistency in closing out games remains a problem. The Mariners’ save percentage, around league average, indicates that while the talent is there with relievers like Andrés Muñoz, the overall depth and strategic utilization need improvement.\n",
    "\n",
    "A potential solution could involve adding one or two versatile starters who could serve as a bridge or late-game option. These pitchers would enter games after the primary starter reaches the 4th or 5th inning, taking over to finish or bridge depending on the game’s context. This approach leverages the strengths of an expanded rotation while reducing the exposure of less reliable relievers in high-leverage situations. Using starters in this manner has the dual benefit of keeping opposing lineups off balance and minimizing bullpen fatigue over a long season.\n",
    "\n",
    "This strategy would require a shift in the Mariners’ pitcher usage philosophy but could significantly improve their ability to secure close games, similar to how the Rays have successfully employed a “bulk” pitcher following a shorter start. It could mitigate late-game collapses, boosting the team’s away record and overall performance.\n",
    "\n",
    "Moreover, adding a hitter with the profile of a Vinnie Pasquantino—contact-oriented with a high on-base percentage—would stabilize the lineup. While the rotation is impressive, adding versatile pitching and hitting options would better position the Mariners to close games and capitalize on offensive opportunities, reducing late-game issues that have plagued them this season.\n",
    "\"\"\")\n",
    "    \n",
    "# Main function for app navigation and control\n",
    "def main():\n",
    "    st.sidebar.title(\"Navigation\")\n",
    "    options = st.sidebar.radio(\"Select Section\", [\"Info\", \"Prediction\", \"Scouting Report\", \"Mariners Improvements\"])\n",
    "\n",
    "    if options == \"Info\":\n",
    "        info_section()\n",
    "    elif options == \"Prediction\":\n",
    "        prediction_section()\n",
    "    elif options == \"Scouting Report\":\n",
    "        scouting_report_section()\n",
    "    elif options == \"Mariners Improvements\":\n",
    "        mariners_improvements_section()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_baseball",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
